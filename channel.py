#!/usr/bin/env python3
"""
Channel processing module for customer support message segmentation.

This module contains the Channel class that handles:
- Message segmentation into chunks for LLM analysis
- Case generation and classification
- Results validation and output
"""

import os
import pandas as pd  # type: ignore
from typing import List, Dict, Any, TYPE_CHECKING, Optional
from case import Case, MetaInfo, CasesSegmentationListLLMRes
from utils import Utils
import copy
from collections import defaultdict

if TYPE_CHECKING:
    from llm_client import LLMClient

from vision_processor import VisionProcessor


class Channel:
    """
    Segments processed messages into chunks for LLM analysis.
    Assumes single channel input.
    
    Features:
    - Half-open intervals: Uses [start, end) to avoid boundary duplication
    - Direct chunking: Simple chunk_size-based segmentation
    - Case merging: Handles pairwise merge and global aggregation
    """
    
    # Case schema and anchor constants
    REQUIRED_FIELDS_DEFAULTS = {
        "summary": "N/A",
        "status": "ongoing",            # ç¼ºçœè®¾ä¸º ongoingï¼Œä¾¿äºä¿å®ˆæ‰¿æ¥
        "pending_party": "N/A",
        "last_update": "N/A",
        "segmentation_confidence": 0.0,
        "anchors": {}
    }
    
    ANCHOR_KEYS_STRICT = ("tracking", "order", "buyer", "topic")
    ANCHOR_KEYS_LAX = ("tracking", "order", "order_ids", "buyer", "buyers", "topic")
    
    def __init__(self, df_clean: pd.DataFrame, channel_url: str, session: str, chunk_size: int = 80, overlap: int = 20, enable_classification: bool = True, enable_vision_processing: bool = True, enable_find_sop: bool = True):
        self.df_clean = df_clean.copy()  # Make a copy to avoid modifying original
        self.channel_url = channel_url
        self.session = session
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.enable_classification = enable_classification
        self.enable_vision_processing = enable_vision_processing
        self.enable_find_sop = enable_find_sop
        self.cases: List[Case] = []
        
        self.validate_parameters()
    
    def validate_parameters(self) -> None:
        """Validate chunk_size and overlap parameters"""
        if self.chunk_size <= 0:
            raise ValueError("chunk_size must be positive")
        if self.overlap < 0:
            raise ValueError("overlap must be non-negative")
        if self.overlap >= self.chunk_size / 4:
            raise ValueError(f"overlap ({self.overlap}) must be < chunk_size/4 ({self.chunk_size/4:.1f})")
    
    
    
    def segment_all_chunks(self, llm_client: 'LLMClient') -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆchunkså¹¶è¿›è¡ŒLLM case segmentationï¼Œè¿”å›ä¿®å¤åçš„casesåˆ—è¡¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            
        Returns:
            ä¿®å¤åçš„caseå­—å…¸åˆ—è¡¨
        """
        # Generate chunks internally
        total_messages = len(self.df_clean)
        
        # Calculate number of chunks needed
        import math
        num_chunks = math.ceil(total_messages / self.chunk_size)
        
        print(f"        ğŸ“¦ Processing {num_chunks} chunks for LLM segmentation")
        
        raw_cases = []
        for i in range(num_chunks):
            # Calculate chunk boundaries using half-open intervals
            start_idx = i * self.chunk_size
            end_idx = min((i + 1) * self.chunk_size, total_messages)
            
            # Create chunk DataFrame slice
            chunk_messages = self.df_clean.iloc[start_idx:end_idx].copy()
            
            print(f"            Chunk {i+1}/{num_chunks}: Processing chunk {i}")
            
            # Format messages using Utils helper method
            current_messages = Utils.format_messages_for_prompt(chunk_messages)
            
            # Generate case segments using LLM for current chunk
            try:
                # Load the segmentation prompt template
                prompt_template = llm_client.load_prompt("segmentation_prompt.md")
            except FileNotFoundError as e:
                raise RuntimeError(f"Cannot load segmentation prompt: {e}")
            
            final_prompt = prompt_template.replace(
                "<<<INSERT_CHUNK_BLOCK_HERE>>>", 
                current_messages
            )
            
            # Generate case segments using LLM
            try:
                # Generate contextual call label with timestamp                
                structured_response = llm_client.generate_structured(
                    final_prompt, 
                    CasesSegmentationListLLMRes, 
                    call_label=f"case_segmentation_{Utils.format_channel_for_display(self.channel_url)}_chunk_{i}"
                )
                    
                # structured_response is already a dict, extract complete_cases
                chunk_raw_cases = structured_response['complete_cases']
                raw_cases.extend(chunk_raw_cases)
                
            except Exception as e:
                raise RuntimeError(f"Failed to generate case segments for chunk {i}: {e}")
        
        print(f"        âœ… LLM segmentation complete ({len(raw_cases)} raw cases collected)")
        
        # Repair all cases with unified logic (repair method includes internal reporting)
        print(f"        ğŸ”§ Running unified repair on {len(raw_cases)} raw cases")
        repair_result = self.repair_case_segment_output(
            cases=raw_cases,
            chunk_df=self.df_clean,  # Use full dataframe for repair
            prev_context=None
        )
        
        repaired_cases = repair_result['cases_out']
        print(f"    âœ… Segmentation and repair complete ({len(repaired_cases)} repaired cases)")
        return repaired_cases
    
    
    def build_cases_via_llm(self, llm_client: 'LLMClient') -> List[Case]:
        """
        æ„å»ºcasesï¼šç›´æ¥å¯¹channel messagesè¿›è¡Œåˆ†å‰²ï¼Œåˆ›å»ºCaseå¯¹è±¡å¹¶åˆ†ç±»
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            
        Returns:
            Caseå¯¹è±¡åˆ—è¡¨ï¼ŒåŒ…å«åˆ†ç±»å’Œæ€§èƒ½æŒ‡æ ‡
        """
        print(f"    ğŸ”„ Segmenting channel messages directly")
        
        # 0. Process vision analysis if enabled
        if self.enable_vision_processing:
            try:
                self.process_file_type_messages(llm_client)
            except Exception as e:
                print(f"        âš ï¸  Vision processing failed: {e}")
                print(f"            Continuing without vision processing...")
        
        # 1. ç›´æ¥å¯¹æ•´ä¸ªchannelçš„æ¶ˆæ¯è¿›è¡Œåˆ†å‰²
        repaired_case_dicts = self.segment_all_chunks(llm_client)
        
        print(f"    ğŸ—ï¸  Creating Case objects with classification and metrics")
        
        # 2. å°†å­—å…¸è½¬æ¢ä¸ºCaseå¯¹è±¡ï¼Œå¹¶æ·»åŠ åˆ†ç±»å’ŒæŒ‡æ ‡
        case_objects = []
        for idx, case_dict in enumerate(repaired_case_dicts):
            # Extract messages first using msg_index_list from dictionary
            msg_index_list = case_dict['msg_index_list']
            case_messages = self.df_clean[self.df_clean['msg_ch_idx'].isin(msg_index_list)].copy()
            
            # Create Case object from dictionary
            case_obj = Case(
                case_id=f'case_{idx:03d}',
                channel_url=self.channel_url,
                msg_index_list=msg_index_list,
                messages=case_messages,
                summary=case_dict['summary'],
                status=case_dict['status'],
                pending_party=case_dict['pending_party'],
                segmentation_confidence=case_dict['segmentation_confidence'],
                meta=MetaInfo(
                    tracking_numbers=case_dict.get('meta', {}).get('tracking_numbers', []),
                    order_numbers=case_dict.get('meta', {}).get('order_numbers', []),
                    user_names=case_dict.get('meta', {}).get('user_names', [])
                )
            )
                        
            # Perform classification using LLM
            try:
                if self.enable_classification == True:
                    print(f"        ğŸ“Š Classifying case {case_obj.case_id}")
                    case_obj.classify_case(llm_client)
            except Exception as e:
                print(f"        âš ï¸  Classification failed for {case_obj.case_id}: {e}")

            # Perform SOP finding
            try:
                if self.enable_find_sop == True:
                    print(f"        ğŸ” Finding SOP for case {case_obj.case_id}")
                    case_obj.find_sop()
            except Exception as e:
                print(f"        âš ï¸  SOP finding failed for {case_obj.case_id}: {e}")

            case_objects.append(case_obj)
        
        self.cases = case_objects
        
        print(f"    âœ… Cases built successfully ({len(self.cases)} Case objects)")
        return self.cases
    
    def build_cases_via_file(self, output_dir: str, llm_client: 'LLMClient') -> List[Case]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½ç°æœ‰ç»“æœå¹¶æ„å»ºCaseå¯¹è±¡ï¼Œå¯é€‰è¿›è¡ŒLLMåˆ†ç±»å’ŒSOPæŸ¥æ‰¾

        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            llm_client: LLMå®¢æˆ·ç«¯ï¼Œç”¨äºåˆ†ç±»å’ŒSOPæŸ¥æ‰¾

        Returns:
            Caseå¯¹è±¡åˆ—è¡¨ï¼ŒåŒ…å«ä»æ–‡ä»¶åŠ è½½çš„æ‰€æœ‰æ•°æ®
        """
        import json
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸save_results_to_jsonç›¸åŒçš„é€»è¾‘ï¼‰
        session_folder = os.path.join(output_dir, f"session_{self.session}")
        channel_name = Utils.format_channel_for_display(self.channel_url)
        channel_cases_file = os.path.join(session_folder, f"cases_{channel_name}.json")
        
        if not os.path.exists(channel_cases_file):
            raise FileNotFoundError(f"JSON file not found: {channel_cases_file}")
        
        # åŠ è½½JSONæ•°æ®
        try:
            with open(channel_cases_file, 'r', encoding='utf-8') as f:
                global_cases_data = json.load(f)
        except Exception as e:
            raise RuntimeError(f"Failed to load JSON file: {e}")
        
        # å°†å­—å…¸æ•°æ®è½¬æ¢ä¸ºCaseå¯¹è±¡
        case_objects = []
        for case_dict in global_cases_data:
            # åˆ›å»ºCaseå¯¹è±¡ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®
            msg_index_list = case_dict.get('msg_index_list', [])


            # åŠ è½½Classificationç›¸å…³å­—æ®µ
            has_classification = case_dict.get('has_classification', False)
            main_category = case_dict.get('main_category', 'unknown')
            sub_category = case_dict.get('sub_category', 'unknown')
            classification_reasoning = case_dict.get('classification_reasoning', 'N/A')
            classification_confidence = case_dict.get('classification_confidence', 0.0)
            classification_indicators = case_dict.get('classification_indicators', [])                

            # åŠ è½½SOPç›¸å…³å­—æ®µ
            has_sop = case_dict.get('has_sop', False)
            sop_content = case_dict.get('sop_content', 'N/A')
            sop_url = case_dict.get('sop_url', 'N/A')
            sop_score = case_dict.get('sop_score', 0.0)

            case_obj = Case(
                case_id=case_dict.get('case_id'),
                msg_index_list=msg_index_list,
                messages=pd.DataFrame(case_dict.get('messages', [])),
                summary=case_dict.get('summary', 'N/A'),
                status=case_dict.get('status', 'ongoing'),
                pending_party=case_dict.get('pending_party', 'N/A'),
                segmentation_confidence=case_dict.get('segmentation_confidence', 0.0),
                channel_url=case_dict.get('channel_url', self.channel_url),
                # åŠ è½½åˆ†ç±»ç»“æœï¼ˆåŸºäºhas_classificationå†³å®šï¼‰
                main_category=main_category,
                sub_category=sub_category,
                classification_reasoning=classification_reasoning,
                classification_confidence=classification_confidence,
                classification_indicators=classification_indicators,
                has_classification=has_classification,
                # åŠ è½½SOPä¿¡æ¯
                has_sop=has_sop,
                sop_content=sop_content,
                sop_url=sop_url,
                sop_score=sop_score,
                # åŠ è½½metaä¿¡æ¯
                meta=MetaInfo(
                    tracking_numbers=case_dict.get('meta', {}).get('tracking_numbers', []),
                    order_numbers=case_dict.get('meta', {}).get('order_numbers', []),
                    user_names=case_dict.get('meta', {}).get('user_names', [])
                )
            )

            if self.enable_classification == True and case_obj.has_classification == False:
                try:
                    print(f"        ğŸ“Š Classifying case {case_obj.case_id}")
                    case_obj.classify_case(llm_client)
                except Exception as e:
                    print(f"        âš ï¸  Classification failed for {case_obj.case_id}: {e}")

            if self.enable_find_sop == True and case_obj.has_sop == False:
                try:
                    print(f"        ğŸ” Finding SOP for case {case_obj.case_id}")
                    case_obj.find_sop()
                except Exception as e:
                    print(f"        âš ï¸  SOP finding failed for {case_obj.case_id}: {e}")


            case_objects.append(case_obj)

        self.cases = case_objects
        
        print(f"        âœ… Cases loaded from file successfully ({len(self.cases)} Case objects)")
        return self.cases

    def classify_all_cases_via_llm(self, llm_client: 'LLMClient') -> Dict[str, Any]:
        """
        å¯¹æ‰€æœ‰å·²åŠ è½½çš„casesè¿›è¡ŒLLMåˆ†ç±»
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            
        Returns:
            åˆ†ç±»ç»“æœç»Ÿè®¡å­—å…¸ï¼ŒåŒ…å«æˆåŠŸ/å¤±è´¥æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯
        """
        if not self.cases:
            raise ValueError("No cases loaded. Call build_cases_via_llm or build_cases_via_file first.")
        
        print(f"    ğŸ“Š Classifying {len(self.cases)} cases via LLM...")
        
        results = {
            "total_cases": len(self.cases),
            "successful_classifications": 0,
            "failed_classifications": 0,
            "failures": []
        }
        
        for case_obj in self.cases:
            print(f"        ğŸ“Š Classifying case {case_obj.case_id}")
            try:
                case_obj.classify_case(llm_client)
                results["successful_classifications"] += 1
            except Exception as e:
                print(f"        âš ï¸  Classification failed for {case_obj.case_id}: {e}")
                results["failed_classifications"] += 1
                results["failures"].append({
                    "case_id": case_obj.case_id,
                    "error": str(e)
                })
        
        success_rate = (results["successful_classifications"] / results["total_cases"]) * 100
        print(f"    âœ… Classification complete: {results['successful_classifications']}/{results['total_cases']} successful ({success_rate:.1f}%)")
        
        return results

    def classify_all_cases_via_file(self, output_dir: str) -> Dict[str, Any]:
        """
        ä»JSONæ–‡ä»¶æ›´æ–°æ‰€æœ‰casesçš„åˆ†ç±»ä¿¡æ¯ï¼Œä¸è¿›è¡ŒLLMè°ƒç”¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            æ›´æ–°ç»“æœç»Ÿè®¡å­—å…¸ï¼ŒåŒ…å«æˆåŠŸ/å¤±è´¥æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯
        """
        if not self.cases:
            raise ValueError("No cases loaded. Call build_cases_via_llm or build_cases_via_file first.")
        
        import json
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        session_folder = os.path.join(output_dir, f"session_{self.session}")
        channel_name = Utils.format_channel_for_display(self.channel_url)
        channel_cases_file = os.path.join(session_folder, f"cases_{channel_name}.json")
        
        if not os.path.exists(channel_cases_file):
            raise FileNotFoundError(f"JSON file not found: {channel_cases_file}")
        
        print(f"    ğŸ“„ Updating {len(self.cases)} cases classification from file...")
        
        # åŠ è½½JSONæ•°æ®
        try:
            with open(channel_cases_file, 'r', encoding='utf-8') as f:
                file_cases_data = json.load(f)
        except Exception as e:
            raise RuntimeError(f"Failed to load JSON file: {e}")
        
        # åˆ›å»ºcase_idåˆ°æ–‡ä»¶æ•°æ®çš„æ˜ å°„
        file_cases_by_id = {case_data.get('case_id'): case_data for case_data in file_cases_data}
        
        results = {
            "total_cases": len(self.cases),
            "updated_cases": 0,
            "not_found_in_file": 0,
            "not_found_cases": []
        }
        
        for case_obj in self.cases:
            case_id = case_obj.case_id
            if case_id in file_cases_by_id:
                file_case_data = file_cases_by_id[case_id]
                
                # æ›´æ–°åˆ†ç±»ç›¸å…³å­—æ®µ
                case_obj.main_category = file_case_data.get('main_category', 'unknown')
                case_obj.sub_category = file_case_data.get('sub_category', 'unknown')
                case_obj.classification_reasoning = file_case_data.get('classification_reasoning', 'N/A')
                case_obj.classification_confidence = file_case_data.get('classification_confidence', 0.0)
                case_obj.classification_indicators = file_case_data.get('classification_indicators', [])
                
                results["updated_cases"] += 1
                print(f"        ğŸ“„ Updated classification for case {case_id}")
            else:
                results["not_found_in_file"] += 1
                results["not_found_cases"].append(case_id)
                print(f"        âš ï¸  Case {case_id} not found in file data")
        
        update_rate = (results["updated_cases"] / results["total_cases"]) * 100
        print(f"    âœ… Classification update complete: {results['updated_cases']}/{results['total_cases']} updated ({update_rate:.1f}%)")
        
        return results

    def save_results_to_json(self, output_dir: str) -> None:
        """Save channel cases to JSON file"""
        import json
        
        # Create session folder for organized output
        session_folder = os.path.join(output_dir, f"session_{self.session}")
        os.makedirs(session_folder, exist_ok=True)
        
        # Save channel cases to JSON in session folder
        channel_name = Utils.format_channel_for_display(self.channel_url)
        channel_cases_file = os.path.join(session_folder, f"cases_{channel_name}.json")
        save_result = [case.to_dict() for case in self.cases]
        
        try:
            with open(channel_cases_file, 'w', encoding='utf-8') as f:
                json.dump(save_result, f, indent=2, ensure_ascii=False)
            print(f"            Channel cases saved to: {channel_cases_file}")
        except IOError as e:
            print(f"                âŒ Error saving JSON file: {e}")
            raise
    
    def save_results_to_csv(self, output_dir: str) -> None:
        """Save annotated messages to CSV file"""
        # Generate annotated CSV for this channel
        df_annotated = self.df_clean.copy()
        df_annotated['case_id'] = "unassigned"  # Default: unassigned (string type)
        # Add classification columns (only main_category and sub_category)
        df_annotated['main_category'] = "unknown"
        df_annotated['sub_category'] = "unknown"
        df_annotated['sop_url'] = "N/A"

        # Map case assignments and classification data using msg_ch_idx
        for case_obj in self.cases:
            case_id = case_obj.case_id or "unknown"
            main_category = case_obj.main_category
            sub_category = case_obj.sub_category
            sop_url = case_obj.sop_url

            for msg_ch_idx in case_obj.msg_index_list:
                mask = df_annotated['msg_ch_idx'] == msg_ch_idx
                df_annotated.loc[mask, 'case_id'] = case_id
                df_annotated.loc[mask, 'main_category'] = main_category
                df_annotated.loc[mask, 'sub_category'] = sub_category
                df_annotated.loc[mask, 'sop_url'] = sop_url
        
        # Create session folder for organized output (same folder as JSON)
        session_folder = os.path.join(output_dir, f"session_{self.session}")
        os.makedirs(session_folder, exist_ok=True)
        
        # Save annotated CSV for this channel in session folder
        channel_name = Utils.format_channel_for_display(self.channel_url)
        channel_segmented_file = os.path.join(session_folder, f"segmented_{channel_name}.csv")
        try:
            df_annotated.to_csv(channel_segmented_file, index=False, encoding='utf-8')
            print(f"            Channel annotated CSV saved to: {channel_segmented_file}")
        except IOError as e:
            print(f"                âŒ Error saving CSV file: {e}")
            raise

    def repair_case_segment_output(self, cases: List[Dict[str, Any]], 
                                 chunk_df: pd.DataFrame,
                                 prev_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ª chunk çš„ LLM åˆ†æ®µç»“æœè¿›è¡Œä¿®å¤ & æ ¡éªŒï¼ˆä¸ä¿®æ”¹å…¥å‚ï¼‰ã€‚
        - å»é‡ï¼šåŒä¸€ msg å‡ºç°åœ¨å¤šä¸ª caseï¼Œåªä¿ç•™ä¸€ä¸ªï¼ˆå¯è§£é‡Šçš„æ‹©ä¸€è§„åˆ™ï¼‰
        - æœªåˆ†é…ï¼šå¿…é¡»æŒ‚é åˆ°åˆç†çš„ caseï¼ˆç©ºæ¶ˆæ¯ä¼˜å…ˆæŒ‚é åˆ°ç›¸åŒsenderçš„æœ€è¿‘æ¶ˆæ¯ï¼‰
        - è¡¥é½å­—æ®µã€æ’åºç¨³å®šã€è‡ªæ£€æŠ¥å‘Š

        Args
        ----
        cases : List[Dict]      # LLM è¾“å‡ºçš„ cases
        prev_context : Optional[Dict]  # ä¸Šä¸€å—å°¾éƒ¨æ‘˜è¦ï¼Œç”¨äºæ‰¿æ¥åˆ¤æ–­

        Returns
        -------
        {
          "cases_out": List[Dict],
          "provisionals": List[Dict],   # å»é‡/è‡ªåŠ¨æŒ‚é çš„è®°å½•ï¼Œä¾¿äºåç»­å¤æ ¸
          "report": { ... }             # è‡ªæ£€ç»Ÿè®¡
        }
        """
        # å†…éƒ¨helperå‡½æ•°
        def _ensure_case_schema(c: Dict[str, Any]) -> Dict[str, Any]:
            """è¡¥é½å­—æ®µã€è§„èŒƒç±»å‹ï¼Œä¸æ”¹å…¥å‚ï¼ˆåœ¨å¤–å±‚ä¼š deepcopyï¼‰"""
            # Import from ChannelSegmenter constants
            REQUIRED_FIELDS_DEFAULTS = {
                "summary": "N/A",
                "status": "ongoing",
                "pending_party": "N/A",
                "segmentation_confidence": 0.0,
                "meta": {
                    "tracking_numbers": [],
                    "order_numbers": [],
                    "user_names": []
                }
            }
            # Legacy anchor keys to meta field mapping
            ANCHOR_TO_META_MAPPING = {
                "tracking": "tracking_numbers",
                "order": "order_numbers", 
                "order_ids": "order_numbers",
                "buyer": "user_names",
                "buyers": "user_names"
            }
            
            if "msg_index_list" not in c or not isinstance(c["msg_index_list"], list):
                c["msg_index_list"] = []
            # ç»Ÿä¸€æ•´å‹ + å‡åºå»é‡
            c["msg_index_list"] = sorted({int(x) for x in c["msg_index_list"]})

            for k, v in REQUIRED_FIELDS_DEFAULTS.items():
                c.setdefault(k, copy.deepcopy(v))

            # å¤„ç†legacy anchorså­—æ®µï¼Œè½¬æ¢ä¸ºmetaç»“æ„
            if "anchors" in c and isinstance(c["anchors"], dict):
                # è¿ç§»anchorsåˆ°meta
                for anchor_key, meta_key in ANCHOR_TO_META_MAPPING.items():
                    anchor_value = c["anchors"].get(anchor_key)
                    if anchor_value is not None:
                        # ç»Ÿä¸€ä¸º list[str]
                        if isinstance(anchor_value, (str, int)):
                            meta_list = [str(anchor_value)]
                        elif isinstance(anchor_value, list):
                            meta_list = [str(x) for x in anchor_value if x is not None]
                        else:
                            meta_list = [str(anchor_value)]
                        
                        # åˆå¹¶åˆ°metaå­—æ®µï¼Œé¿å…é‡å¤
                        existing = set(c["meta"][meta_key])
                        c["meta"][meta_key] = list(existing.union(meta_list))
                
                # åˆ é™¤legacy anchorså­—æ®µ
                del c["anchors"]

            # ç¡®ä¿metaå­—æ®µç»“æ„æ­£ç¡®
            if not isinstance(c["meta"], dict):
                c["meta"] = copy.deepcopy(REQUIRED_FIELDS_DEFAULTS["meta"])
            
            # ç¡®ä¿metaä¸­çš„æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
            for field in ["tracking_numbers", "order_numbers", "user_names"]:
                if field not in c["meta"]:
                    c["meta"][field] = []
                elif not isinstance(c["meta"][field], list):
                    c["meta"][field] = []

            # ç½®ä¿¡åº¦è£å‰ª
            try:
                c["segmentation_confidence"] = float(c.get("segmentation_confidence", 0.0))
            except Exception:
                c["segmentation_confidence"] = 0.0
            c["segmentation_confidence"] = max(0.0, min(1.0, c["segmentation_confidence"]))

            # çŠ¶æ€åˆæ³•æ€§
            if c.get("status") not in ("open", "ongoing", "resolved", "blocked"):
                c["status"] = "ongoing"

            return c
        
        def _anchor_strength(case: Dict[str, Any]) -> int:
            # tracking(4) > order(3) > buyer(2)
            meta = case.get("meta", {})
            if meta.get("tracking_numbers"): return 4
            if meta.get("order_numbers"): return 3
            if meta.get("user_names"): return 2
            return 0

        def _hits_active_hints(case: Dict[str, Any], prev_context: Optional[Dict[str, Any]]) -> bool:
            if not prev_context: return False
            hints = prev_context.get("ACTIVE_CASE_HINTS", [])
            if not hints: return False
            meta = case.get("meta", {})
            
            # Check meta fields against hints
            meta_fields = ["tracking_numbers", "order_numbers", "user_names"]
            for h in hints:
                h_meta = h.get("meta", {})
                for field in meta_fields:
                    case_values = set(meta.get(field, []))
                    hint_values = set(h_meta.get(field, []))
                    if case_values & hint_values:  # Intersection
                        return True
            return False

        def _proximity_score(i: int, case: Dict[str, Any]) -> float:
            ml = case.get("msg_index_list", [])
            if not ml: return 0.0
            # msg_index_list is still indices at repair stage
            dist = min(abs(i - m) for m in ml)
            return 1.0 / (1 + dist)  # 1, 0.5, 0.33, ...

        def _choose_one_for_duplicate(i: int, cases: List[Dict[str, Any]], cids: List[int], prev_context: Optional[Dict[str, Any]]) -> int:
            # è§„åˆ™ï¼šanchor_strength > æ‰¿æ¥(prev_context) > segmentation_confidence > proximity > è¾ƒå° case_idï¼ˆç¨³å®šï¼‰
            scored = []
            for cid in cids:
                c = cases[cid]
                scored.append((
                    _anchor_strength(c),
                    1 if _hits_active_hints(c, prev_context) else 0,
                    float(c.get("segmentation_confidence", 0.0)),
                    _proximity_score(i, c),
                    -cid,  # åå‘ç”¨äºæœ€åçš„ç¨³å®š tie-breakï¼ˆè¶Šå°ä¼˜å…ˆï¼‰
                    cid
                ))
            scored.sort(reverse=True)
            return scored[0][-1]

        def _is_empty_message(msg_idx: int) -> bool:
            """æ£€æŸ¥æ¶ˆæ¯å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–ç©ºç™½"""
            if msg_idx not in chunk_df['msg_ch_idx'].values:
                return True
            message = chunk_df[chunk_df['msg_ch_idx'] == msg_idx].iloc[0]
            text = str(message.get('Message', '')).strip()  # Use 'Message' column as seen in Utils.format_messages_for_prompt
            return len(text) == 0
        
        def _find_nearest_same_sender_case(msg_idx: int, cases: List[Dict]) -> Optional[int]:
            """æŸ¥æ‰¾åŒ…å«æœ€è¿‘çš„ç›¸åŒsender_idæ¶ˆæ¯çš„case"""
            if msg_idx not in chunk_df['msg_ch_idx'].values:
                return None
            
            target_sender = chunk_df[chunk_df['msg_ch_idx'] == msg_idx].iloc[0].get('Sender ID', '')
            if not target_sender:
                return None
            
            # æ„å»ºæ¶ˆæ¯åˆ°caseçš„æ˜ å°„
            msg_to_case = {}
            for case_idx, case in enumerate(cases):
                for msg_id in case.get('msg_index_list', []):
                    msg_to_case[msg_id] = case_idx
            
            # å¯»æ‰¾æœ€è¿‘çš„ç›¸åŒsenderæ¶ˆæ¯
            best_distance = float('inf')
            best_case_id = None
            
            for check_msg_idx, case_idx in msg_to_case.items():
                if check_msg_idx in chunk_df['msg_ch_idx'].values:
                    check_sender = chunk_df[chunk_df['msg_ch_idx'] == check_msg_idx].iloc[0].get('Sender ID', '')
                    if check_sender == target_sender:
                        distance = abs(check_msg_idx - msg_idx)
                        if distance < best_distance:
                            best_distance = distance
                            best_case_id = case_idx
            
            return best_case_id
        
        def _attach_unassigned_smart(msg_idx: int, cases: List[Dict]) -> Optional[int]:
            """æ™ºèƒ½æŒ‚é é€»è¾‘ï¼ˆåŸºäºåŸ_attach_unassigned_simpleï¼‰"""
            scored = []
            for cid, c in enumerate(cases):
                # ä»…è€ƒè™‘ open/ongoing/blocked çš„ï¼Œè·³è¿‡ resolved
                if c.get("status") == "resolved":
                    continue
                scored.append((
                    1 if _hits_active_hints(c, prev_context) else 0,
                    _anchor_strength(c),
                    _proximity_score(msg_idx, c),
                    float(c.get("segmentation_confidence", 0.0)),
                    -cid,
                    cid
                ))
            if not scored:
                return None
            scored.sort(reverse=True)
            # å¦‚æœå®Œå…¨æ²¡æœ‰ä¸Šä¸‹æ–‡å‘½ä¸­ä¸”é”šç‚¹/è´´è¿‘éƒ½å¾ˆå¼±ï¼Œå¯ä»¥é˜ˆå€¼ä¸¢å¼ƒï¼Œé¿å…è¯¯æŒ‚
            best = scored[0]
            cont, anc, prox, _, _, cid = best
            if (cont == 0 and anc == 0 and prox < 0.25):  # è´´è¿‘åº¦é˜ˆå€¼å¯è°ƒ
                return None
            return cid
        
        def _attach_to_any_nearest_case(msg_idx: int, cases: List[Dict]) -> int:
            """ç»ˆæå…œåº•ï¼šæŒ‚é åˆ°ä»»ä½•æœ€è¿‘çš„case"""
            if not cases:
                return 0  # å¦‚æœæ²¡æœ‰casesï¼Œè¿”å›ç¬¬ä¸€ä¸ªï¼ˆè¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
            
            # æ‰¾åˆ°åŒ…å«æœ€è¿‘æ¶ˆæ¯çš„case
            best_distance = float('inf')
            best_case_id = 0
            
            for case_idx, case in enumerate(cases):
                for msg_id in case.get('msg_index_list', []):
                    distance = abs(msg_id - msg_idx)
                    if distance < best_distance:
                        best_distance = distance
                        best_case_id = case_idx
            
            return best_case_id
        
        def _attach_to_case(msg_idx: int, case_id: int, cases: List[Dict], provisionals: List[Dict], reason: str):
            """æ‰§è¡ŒæŒ‚é æ“ä½œ"""
            if case_id < len(cases):
                cases[case_id]["msg_index_list"].append(msg_idx)
                cases[case_id]["msg_index_list"] = sorted(set(cases[case_id]["msg_index_list"]))
                provisionals.append({
                    "type": "auto_attach",
                    "msg_idx": msg_idx,
                    "attached_to": case_id,
                    "reason": reason
                })
        
        # ä½¿ç”¨è‡ªå·±çš„æ¶ˆæ¯IDåˆ—è¡¨
        chunk_msg_ids = chunk_df['msg_ch_idx'].tolist()
            
        out = copy.deepcopy(cases)

        # 1) è§„èŒƒåŒ– & è¡¥é½å­—æ®µ
        for idx in range(len(out)):
            out[idx] = _ensure_case_schema(out[idx])

        # 2) case å†…æ’åºç¨³å®š + å»ç©º case
        out = [c for c in out if c["msg_index_list"]]
        out.sort(key=lambda c: (c["msg_index_list"][0], c.get("segmentation_confidence", 0.0) * -1))

        # 3) å»ºç«‹ msg -> cases åæŸ¥
        msg_to_cases = defaultdict(list)
        for cid, c in enumerate(out):
            for i in c["msg_index_list"]:
                msg_to_cases[i].append(cid)

        provisionals = []

        # 4) å»é‡ï¼šæ¯æ¡ msg åªä¿ç•™ä¸€ä¸ª case
        for i, cids in list(msg_to_cases.items()):
            if len(cids) <= 1:
                continue
            winner = _choose_one_for_duplicate(i, out, cids, prev_context)
            losers = [cid for cid in cids if cid != winner]
            # ä» loser ä¸­ç§»é™¤è¯¥ msg
            for cid in losers:
                ml = out[cid]["msg_index_list"]
                if i in ml:
                    out[cid]["msg_index_list"] = [x for x in ml if x != i]
            provisionals.append({
                "type": "duplicate_resolution",
                "msg_idx": i,
                "chosen_case": winner,
                "rejected_cases": losers,
                "reason": "anchor > continuation > segmentation_confidence > proximity > case_id"
            })

        # 5) å†æ¬¡æ¸…ç†ç©º case
        out = [c for c in out if c["msg_index_list"]]
        # é‡æ–°æ„å»º msg ç´¢å¼•
        msg_to_cases.clear()
        for cid, c in enumerate(out):
            for i in c["msg_index_list"]:
                msg_to_cases[i].append(cid)

        # 6) è¯†åˆ«æœªåˆ†é…
        chunk_set = set(int(x) for x in chunk_msg_ids)
        assigned = set(msg_to_cases.keys())
        unassigned = sorted(list(chunk_set - assigned))

        # 7) æŒ‚é æœªåˆ†é…ï¼ˆå¿…é¡»å…¨éƒ¨æŒ‚é ï¼Œä¸‰å±‚ä¼˜å…ˆçº§ï¼‰
        for i in unassigned:
            cid = None
            reason = ""
            
            # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç©ºæ¶ˆæ¯æŒ‚é åˆ°ç›¸åŒsenderçš„æœ€è¿‘æ¶ˆæ¯
            if _is_empty_message(i):
                cid = _find_nearest_same_sender_case(i, out)
                if cid is not None:
                    reason = "empty_message_same_sender"
            
            # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ™ºèƒ½æŒ‚é é€»è¾‘
            if cid is None:
                cid = _attach_unassigned_smart(i, out)
                if cid is not None:
                    reason = "smart_attachment"
            
            # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šç»ˆæå…œåº•æŒ‚é 
            if cid is None:
                cid = _attach_to_any_nearest_case(i, out)
                reason = "nearest_fallback"
            
            # æ‰§è¡ŒæŒ‚é ï¼ˆcidä¿è¯ä¸ä¸ºNoneï¼‰
            _attach_to_case(i, cid, out, provisionals, reason)

        # 8) æœ€ç»ˆæ’åºç¨³å®šï¼ˆæŒ‰æœ€å° msg å‡åºï¼‰
        out.sort(key=lambda c: c["msg_index_list"][0])

        # 9) è‡ªæ£€æŠ¥å‘Š
        # 9.1 ç»Ÿè®¡é‡å¤ä¸è¦†ç›–ç‡
        final_msg_to_cases = defaultdict(int)
        for c in out:
            for i in c["msg_index_list"]:
                final_msg_to_cases[i] += 1
        duplicates_after = [i for i, cnt in final_msg_to_cases.items() if cnt > 1]
        covered = set(final_msg_to_cases.keys())
        missing_after = sorted(list(chunk_set - covered))

        report = {
            "total_cases_in": len(cases),
            "total_cases_out": len(out),
            "total_msgs": len(chunk_set),
            "covered_msgs": len(covered),
            "missing_msgs": len(missing_after),
            "duplicates_after": len(duplicates_after),
            "duplicates_after_list": duplicates_after[:50],  # æˆªæ–­ï¼Œé¿å…è¿‡å¤§
        }

        # æ‰“å°ä¿®å¤æƒ…å†µæŠ¥å‘Š
        if provisionals:
            print(f"        ğŸ”§ Applied {len(provisionals)} repair actions:")
            for prov in provisionals:
                if prov['type'] == 'duplicate_resolution':
                    print(f"            âœ Resolved duplicate msg {prov['msg_idx']}: kept in case {prov['chosen_case']}")
                elif prov['type'] == 'auto_attach':
                    print(f"            â• Auto-attached msg {prov['msg_idx']} to case {prov['attached_to']}")
                elif prov['type'] == 'misc_bucket':
                    print(f"            ğŸ“¦ Created misc case for {len(prov['msg_idxs'])} unassigned messages")
        
        # æ‰“å°æœ€ç»ˆéªŒè¯ç»“æœ
        if report['missing_msgs'] == 0 and report['duplicates_after'] == 0:
            print(f"        âœ… Repair completed: 100% coverage achieved - Final: {report['covered_msgs']}/{report['total_msgs']} messages in {report['total_cases_out']} cases")
        else:
            print(f"        âš ï¸ Repair incomplete: Missing: {report['missing_msgs']}, Duplicates: {report['duplicates_after']}")

        return {
            "cases_out": out,
            "provisionals": provisionals,
            "report": report
        }
    
    def process_file_type_messages(self, llm_client: 'LLMClient') -> None:
        """Process FILE messages with vision analysis and populate File Summary column."""
        print("        ğŸ” Processing FILE messages with vision analysis...")
        
        # Find FILE messages with image URLs in this channel
        file_messages = self.df_clean[
            (self.df_clean['Type'] == 'FILE') & 
            (self.df_clean['File URL'].notna())
        ].copy()
        
        if len(file_messages) == 0:
            print("            No image FILE messages found for vision processing")
            return
            
        print(f"            Found {len(file_messages)} image FILE messages to process")
            
        # Process each image
        processed_count = 0
        for idx, (df_idx, file_msg) in enumerate(file_messages.iterrows()):
            try:
                image_url = file_msg['File URL']
                msg_ch_idx = file_msg['msg_ch_idx']
                
                print(f"            Processing image {idx + 1}/{len(file_messages)}: {image_url.split('/')[-1]}")
                
                # Get context for this image using static method
                context_df = VisionProcessor.get_context_for_image(
                    channel_df=self.df_clean,
                    image_msg_ch_idx=msg_ch_idx,
                    context_size=3  # Smaller context for batch processing
                )
                
                # Analyze image with context using class method
                analysis_result = VisionProcessor.analyze_image_with_context(
                    context_df=context_df,
                    image_url=image_url,
                    llm_client=llm_client
                )
                
                # Generate synthesized text summary using static method
                summary_text = VisionProcessor.synthesize_visual_text(analysis_result)
                
                # Update File Summary in df_clean
                self.df_clean.loc[df_idx, 'File Summary'] = summary_text
                
                processed_count += 1
                
            except Exception as e:
                print(f"            âš ï¸  Error processing image {image_url}: {e}")
                continue
                
        print(f"            âœ… Successfully processed {processed_count}/{len(file_messages)} images")