#!/usr/bin/env python3
"""
Session management module for customer support message segmentation pipeline.

This module contains the Session class that orchestrates the complete processing pipeline:
- File data processing across all channels
- Individual channel segmentation and case generation  
- Cross-channel statistics generation
- Session-level output management
"""

import os
import json
from datetime import datetime
from typing import List, Optional
import pandas as pd # type: ignore

# Local imports
from channel import Channel
from llm_client import LLMClient
from utils import Utils


class Session:
    """
    Session orchestrator for the complete message processing pipeline.
    
    Manages cross-channel operations, file processing, and session-level outputs.
    Encapsulates all business logic that was previously scattered across main.py and FileProcessor.
    """
    
    def __init__(self, 
                 input_file: str,
                 output_dir: str = 'out', 
                 chunk_size: int = 60,
                 overlap: int = 10,
                 model: str = 'gpt-5',
                 session_name: Optional[str] = None,
                 enable_review: bool = False,
                 enable_vision_processing: bool = True,
                 force_classification: bool = False):
        """
        Initialize session with explicit parameters.
        
        Args:
            input_file: Path to input CSV file (required)
            output_dir: Base output directory (default: 'out')
            chunk_size: Chunk size for segmentation (default: 80)
            overlap: Overlap size between chunks (default: 20)
            model: LLM model name (default: 'gpt-5')
            session_name: Optional session name (auto-generated if None)
            enable_review: Enable case review flag (default: False)
            enable_vision_processing: Enable vision processing for FILE messages (default: False)
            force_classification: Force classification re-run when loading from files (default: False)
        """
        # Pipeline configuration
        self.input_file = input_file
        self.output_dir = output_dir
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.model = model
        self.enable_review = enable_review
        self.enable_vision_processing = enable_vision_processing
        self.force_classification = force_classification
        
        # Session identification and output management
        self.session_name = session_name or datetime.now().strftime("%y%m%d_%H%M%S")
        self.output_folder = os.path.join(output_dir, f"session_{self.session_name}")
        
        # Data processing state
        self.df: Optional[pd.DataFrame] = None
        self.df_clean: Optional[pd.DataFrame] = None
        
        # Cross-channel collections
        self.channels: List[Channel] = []
    
    def cs_mbr(self) -> None:
        """
        Execute the complete processing pipeline.
        
        Pipeline stages:
        1. File data processing (cross-channel)
        2. Session folder setup
        3. Individual channel processing
        4. Export all cases to CSV
        """
        print(f"ğŸš€ Starting pipeline session: {self.session_name}")
        
        try:
            # Stage 1: Process file data across all channels
            if not self.process_file_data():
                print("Error: File processing failed")
                exit(1)
            
            # Stage 2: Create session folder structure
            self.create_session_folder()
            
            # Stage 3: Initialize LLM client and process channels
            self.process_channels()
            
            # Stage 4: Export all cases to CSV
            print(f"\nğŸ“„ Exporting all cases to CSV...")
            try:
                csv_path = self.save_all_cases_to_csv()
                print(f"âœ… CSV export complete: {csv_path}")
            except Exception as e:
                print(f"âš ï¸  CSV export failed: {e}")
            
            print(f"\nâœ… Pipeline complete!")
            
        except Exception as e:
            print(f"Error: {e}")
            exit(1)
    
    def cs_qa(self) -> None:
        """
        Execute QA processing pipeline.
        
        QA-specific pipeline for question answering tasks.
        """
        print(f"ğŸš€ Starting QA pipeline session: {self.session_name}")
        
        try:
            # Stage 1: Process file data across all channels
            if not self.process_file_data():
                print("Error: File processing failed")
                exit(1)
            
            # Stage 2: Create session folder structure
            self.create_session_folder()
            
            # Stage 3: Process only target channels from debug_output
            print(f"\nğŸ”„ Processing target channels...")
            
            # Define target channels from debug_output
            target_channels = [
                "sendbird_group_channel_356380577_45252a653c9cacdde70fb496bd98dea1b0b9cc77",
                "sendbird_group_channel_409455858_71425f1073aa9a8c3a745025409e9e82f8e0cb52",
                "sendbird_group_channel_476430720_c6fe5cbfaf4b3f175abed1e448c2d19b90c52cca",
                "sendbird_group_channel_215482988_c9ac45aa4b3dc2dfd42df0c8bc3b3e080e2a4959",
                "sendbird_group_channel_215482988_dd7d2885f740f3e748edb8b41abbe5f36b0bff2f",
                "sendbird_group_channel_215482988_03e31b162759f2869f99bca09d3d902743e47fe0"
            ]
            
            # Initialize LLM client
            llm_client = LLMClient(model=self.model)
            print(f"LLM Client initialized with model: {self.model}")
            
            # Process only target channels
            for channel_idx, channel_url in enumerate(target_channels, 1):
                if channel_url in self.df_clean['Channel URL'].values:
                    channel_df = self.df_clean[self.df_clean['Channel URL'] == channel_url]
                    print(f"ğŸ”„ Channel {channel_idx}/{len(target_channels)}: {Utils.format_channel_for_display(channel_url)} ({len(channel_df)} messages)")
                    channel = self.build_one_channel(channel_url, llm_client)
                    self.channels.append(channel)
                else:
                    print(f"âš ï¸  Channel {channel_idx}/{len(target_channels)}: {Utils.format_channel_for_display(channel_url)} - not found in data")

            # Stage 4: Find cases matching specific criteria
            print(f"\nğŸ” Filtering cases by category criteria...")
            
            # Collect all cases from processed channels
            all_cases = []
            for channel in self.channels:
                if hasattr(channel, 'cases') and channel.cases:
                    all_cases.extend(channel.cases)
            
            # Filter cases by exact category criteria
            filtered_cases = []
            target_main_category = "Order & Post-sale"
            target_sub_category = "Modify Order / Modify Address / Modify Shipping to local pick up"
            
            for case in all_cases:
                if (case.main_category == target_main_category and 
                    case.sub_category == target_sub_category):
                    filtered_cases.append(case)
            
            # Display results
            print(f"ğŸ” Found {len(filtered_cases)} cases with:")
            print(f"   Main Category: {target_main_category}")
            print(f"   Sub Category: {target_sub_category}")
            print(f"   Processed {len(target_channels)} specific channels from debug_output")
            
            if filtered_cases:
                print(f"\nAll {len(filtered_cases)} matching cases with their messages:")
                for i, case in enumerate(filtered_cases, 1):
                    print(f"  {i}. Case ID: {case.case_id}")
                    print(f"     Channel: {case.channel_url}")
                    print(f"     Status: {case.status}")
                    print(f"     Summary: {case.summary[:100]}...")
                    
                    # Display all messages in this case
                    if case.messages is not None and not case.messages.empty:
                        print(f"     Messages ({len(case.messages)} total):")
                        for msg_idx, msg_row in case.messages.iterrows():
                            message = msg_row.get('Message', 'N/A')
                            role = msg_row.get('role', 'N/A')
                            
                            print(f"       {role}: {message}")
                    else:
                        print(f"     Messages: No messages available")
                    print()
            else:
                print(f"No cases found matching the criteria.")
            
            print(f"\nâœ… QA Pipeline complete!")
            
        except Exception as e:
            print(f"Error: {e}")
            exit(1)
    
    def process_file_data(self) -> bool:
        """
        Process CSV file data and prepare channel data list.
        
        All file processing logic is contained within this method.
        
        Returns:
            True if processing successful, False otherwise
        """
        print("Starting file processing...")
        
        # 1. Load CSV data into DataFrame
        try:
            self.df = pd.read_csv(self.input_file)
            print(f"        Loaded {len(self.df)} messages from {self.input_file}")
        except Exception as e:
            print(f"Error loading file {self.input_file}: {e}")
            return False
        
        # 2. Filter out rows where Deleted = True
        if 'Deleted' in self.df.columns:
            original_count = len(self.df)
            self.df = self.df[self.df['Deleted'] != True].reset_index(drop=True)
            filtered_count = original_count - len(self.df)
            print(f"        Filtered out {filtered_count} deleted rows ({len(self.df)} remaining)")
        else:
            print("No 'Deleted' column found, skipping deletion filter")
        
        # 3. Add role column based on Sender ID pattern
        if 'role' not in self.df.columns:
            self.df['role'] = self.df['Sender ID'].apply(
                lambda x: 'customer_service' if str(x).startswith('psops') else 'user'
            )
            print(f"        Added role column: {self.df['role'].value_counts().to_dict()}")
        else:
            print("Role column already exists, skipping...")
        
        # 4. Parse Created Time to timezone-aware UTC format
        def parse_to_utc(timestamp_str):
            try:
                # Parse with timezone info and convert to UTC
                dt = pd.to_datetime(timestamp_str, utc=True)
                return dt
            except Exception:
                return pd.NaT
        
        self.df['Created Time'] = self.df['Created Time'].apply(parse_to_utc)
        print(f"        Processed time columns, converted {len(self.df)} timestamps to UTC")
        
        # 5. Sort data by Channel URL, Created Time, then Message ID
        self.df = self.df.sort_values([
            'Channel URL',
            'Created Time', 
            'Message ID'
        ]).reset_index(drop=True)
        print(f"        Sorted data by Channel URL, Created Time, and Message ID")
        
        # 6. Add msg_ch_idx column (0..N-1 for each Channel URL group)
        self.df['msg_ch_idx'] = self.df.groupby('Channel URL').cumcount()
        print(f"        Added msg_ch_idx column for {self.df['Channel URL'].nunique()} channels")
        
        # 7. Add File Summary column for vision analysis results
        self.df['File Summary'] = ''
        print(f"        Added File Summary column for storing vision analysis results")
        
        # 8. Generate clean DataFrame with essential columns
        essential_columns = [
            'Created Time', 'Sender ID', 'Message', 'Channel URL',
            'role', 'msg_ch_idx', 'Message ID', 'Type', 'File URL', 'File Summary'
        ]
        available_columns = [col for col in essential_columns if col in self.df.columns]
        self.df_clean = self.df[available_columns].copy()
        print(f"        Created clean DataFrame with {len(available_columns)} columns: {available_columns}")
        
        print(f"        Processed {len(self.df_clean)} messages across {self.df_clean['Channel URL'].nunique()} channels")
        
        # 8. Display channel summary
        for channel_url in self.df_clean['Channel URL'].unique():
            channel_df = self.df_clean[self.df_clean['Channel URL'] == channel_url]
            print(f"                Channel: {Utils.format_channel_for_display(channel_url)} - {len(channel_df)} messages")
        
        return True

    def create_session_folder(self) -> None:
        """Create session output folder structure."""
        os.makedirs(self.output_folder, exist_ok=True)
    
    def build_one_channel(self, channel_url: str, llm_client) -> 'Channel':
        """
        Build and process a single channel.
        
        Args:
            channel_url: The channel URL to process
            llm_client: LLM client instance for processing
            
        Returns:
            Processed Channel instance
        """
        # Extract channel data
        channel_df = self.df_clean[self.df_clean['Channel URL'] == channel_url].copy()
        # Reset msg_ch_idx to ensure it starts from 0 for each channel
        channel_df['msg_ch_idx'] = range(len(channel_df))
        
        # Check if channel results already exist
        channel_name = Utils.format_channel_for_display(channel_url)
        channel_cases_file = os.path.join(self.output_folder, f"cases_{channel_name}.json")
        
        # Create Channel instance
        channel = Channel(channel_df, channel_url, self.session_name, self.chunk_size, self.overlap, self.force_classification, self.enable_vision_processing)
        
        if os.path.exists(channel_cases_file):
            print(f"        â­ï¸  Loading existing results from file")
            channel.build_cases_via_file(self.output_dir)
            
            # Force re-classification if enabled
            if self.force_classification:
                print(f"        ğŸ”„ Force re-classification enabled")
                channel.classify_all_cases_via_llm(llm_client)
            else:
                channel.classify_all_cases_via_file(self.output_dir)
        else:
            # Process channel with full pipeline (includes vision processing if enabled)
            channel.build_cases_via_llm(llm_client)
            channel.classify_all_cases_via_llm(llm_client)
        
        return channel
    
    def process_channels(self) -> None:
        """
        Process each channel individually and collect results.
        
        Migrated from main.py channel processing loop.
        """
        # Initialize LLM client
        llm_client = LLMClient(model=self.model)
        print(f"LLM Client initialized with model: {self.model}")
        
        # Process each channel directly
        channel_urls = self.df_clean['Channel URL'].unique()
        for channel_idx, channel_url in enumerate(channel_urls):
            channel_df = self.df_clean[self.df_clean['Channel URL'] == channel_url].copy()
            
            print(f"ğŸ”„ Channel {channel_idx + 1}/{len(channel_urls)}: {Utils.format_channel_for_display(channel_url)} ({len(channel_df)} messages)")
            
            # Build and process the channel
            channel = self.build_one_channel(channel_url, llm_client)
            
            # Save channel results
            print(f"    ğŸ’¾ Saving results...")
            try:
                channel.save_results_to_json(self.output_dir)
                channel.save_results_to_csv(self.output_dir)
                print(f"    âœ… Results saved successfully")
            except Exception as save_error:
                print(f"        âŒ Error saving results: {str(save_error)}")
                print(f"        Processing completed but save failed - continuing...")
            
            # Collect channel for cross-channel operations
            self.channels.append(channel)
        
        # Summary
        print(f"\nâœ… Pipeline processing complete!")
        print(f"Processed {len(self.channels)} channels")
        print(f"Results saved to timestamped session folders in output directory")
        print(f"Each session contains JSON and CSV files for successfully saved channels")
    
    def save_all_cases_to_csv(self, output_filename: str = None) -> str:
        """
        å¯¼å‡ºæ‰€æœ‰ cases åˆ° CSV æ–‡ä»¶ï¼Œæ ¼å¼ä¸ merge_aug.csv ä¸€è‡´
        
        Args:
            output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰ï¼Œé»˜è®¤ä¸º 'all_cases_merged.csv'
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        import pandas as pd
        
        # æ”¶é›†æ‰€æœ‰ channels çš„ cases
        all_cases = []
        for channel in self.channels:
            if hasattr(channel, 'cases') and channel.cases:
                all_cases.extend(channel.cases)
        
        if not all_cases:
            print("âš ï¸  No cases found to export")
            return ""
        
        print(f"ğŸ“„ Exporting {len(all_cases)} cases to CSV...")
        
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        for case in all_cases:
            # å¤„ç†æ•°ç»„å­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            def format_array_field(field_value):
                if not field_value:
                    return "[]"
                return str(field_value)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ ['item1', 'item2']
            
            # æå–metaä¿¡æ¯
            meta = case.meta if case.meta else None
            tracking_numbers = meta.tracking_numbers if meta else []
            order_numbers = meta.order_numbers if meta else []
            user_names = meta.user_names if meta else []
            
            row = {
                'channel_url': case.channel_url or '',
                'summary': case.summary or '',
                'status': case.status or '',
                'pending_party': case.pending_party or '',
                'segmentation_confidence': case.segmentation_confidence or 0.0,
                'main_category': case.main_category or '',
                'sub_category': case.sub_category or '',
                'classification_confidence': case.classification_confidence or 0.0,
                'first_res_time': case.first_res_time if case.first_res_time != -1 else -1,
                'handle_time': case.handle_time if case.handle_time != -1 else -1,
                'first_contact_resolution': case.first_contact_resolution if case.first_contact_resolution != -1 else -1,
                'usr_msg_num': case.usr_msg_num if case.usr_msg_num != -1 else -1,
                'total_msg_num': case.total_msg_num if case.total_msg_num != -1 else -1,
                'start_time': case.start_time or '',
                'end_time': case.end_time or '',
                'tracking_numbers': format_array_field(tracking_numbers),
                'order_numbers': format_array_field(order_numbers),
                'user_names': format_array_field(user_names),
                'msg_index_list': format_array_field(case.msg_index_list),
                'global_msg_id_list': format_array_field(case.global_msg_id_list)
            }
            csv_data.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(csv_data)
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        session_folder = os.path.join(self.output_dir, f"session_{self.session_name}")
        os.makedirs(session_folder, exist_ok=True)
        
        if output_filename is None:
            output_filename = "all_cases_merged.csv"
        
        output_path = os.path.join(session_folder, output_filename)
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        try:
            df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"            âœ… All cases exported to: {output_path}")
            return output_path
        except Exception as e:
            print(f"            âŒ Error exporting cases to CSV: {e}")
            raise